{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b5d2eea",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "We are going to use the very popular but non-trivial 20 Newsgroups dataset, which is available in Scikit-Learn. The 20 Newsgroups dataset comprises around 18,000 newsgroups posts spread across 20 different categories or topics, thus making it a 20-class classification problem, which is definitely non-trivial as compared to predicting spam in emails. Remember, the higher the number of classes, the more complex it gets to build an accurate classifier. \n",
    "\n",
    "Details pertaining to the dataset can be found at `http://scikit-learn.org/0.19/datasets/twenty_newsgroups.html` and it is recommended to remove the headers, footers, and quotes from the text documents to prevent the model from overfitting or not generalizing well due to certain specific headers or email addresses. \n",
    "\n",
    "Scikit-Learn allows you to load the 20 Newsgroups data and provide a parameter called remove, telling it what kinds of information to strip out of each file. The remove parameter should be a tuple containing any subset of ('headers', 'footers', 'quotes'), telling it to remove headers, signature blocks, and quotation blocks, respectively.\n",
    "\n",
    "We will also remove documents that are empty or have no content after removing these three items during the data preprocessing stage, because it would be pointless to try to extract features from empty documents. \n",
    "\n",
    "### Load data\n",
    "\n",
    "Let’s start by loading the necessary dataset and defining functions for building the training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d1b6fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Elisabetta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Elisabetta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%run setup.ipynb\n",
    "%run text_libraries.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eac9afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46afe07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run text_data_preprocessing_steps.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "459f2445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18846, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Target Label</th>\n",
       "      <th>Target Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>17</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n\\nBack in high school I worked as a lab assi...</td>\n",
       "      <td>12</td>\n",
       "      <td>sci.electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\n\\nAE is in Dallas...try 214/241-6060 or 214/...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n[stuff deleted]\\n\\nOk, here's the solution t...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\n\\n\\nYeah, it's the second one.  And I believ...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\nIf a Christian means someone who believes in...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  Target Label  \\\n",
       "0  \\n\\nI am sure some bashers of Pens fans are pr...            10   \n",
       "1  My brother is in the market for a high-perform...             3   \n",
       "2  \\n\\n\\n\\n\\tFinally you said what you dream abou...            17   \n",
       "3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...             3   \n",
       "4  1)    I have an old Jasmine drive which I cann...             4   \n",
       "5  \\n\\nBack in high school I worked as a lab assi...            12   \n",
       "6  \\n\\nAE is in Dallas...try 214/241-6060 or 214/...             4   \n",
       "7  \\n[stuff deleted]\\n\\nOk, here's the solution t...            10   \n",
       "8  \\n\\n\\nYeah, it's the second one.  And I believ...            10   \n",
       "9  \\nIf a Christian means someone who believes in...            19   \n",
       "\n",
       "                Target Name  \n",
       "0          rec.sport.hockey  \n",
       "1  comp.sys.ibm.pc.hardware  \n",
       "2     talk.politics.mideast  \n",
       "3  comp.sys.ibm.pc.hardware  \n",
       "4     comp.sys.mac.hardware  \n",
       "5           sci.electronics  \n",
       "6     comp.sys.mac.hardware  \n",
       "7          rec.sport.hockey  \n",
       "8          rec.sport.hockey  \n",
       "9        talk.religion.misc  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fetch_20newsgroups(subset='all', shuffle=True,\n",
    "                          remove=('headers', 'footers', 'quotes'))\n",
    "#Downloading 20news dataset. This may take a few minutes.\n",
    "#Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n",
    "data_labels_map = dict(enumerate(data.target_names))\n",
    "\n",
    "# building the dataframe\n",
    "corpus, target_labels, target_names = (data.data, data.target, [data_labels_map[label] for label in data.target])\n",
    "data_df = pd.DataFrame({'Article': corpus, 'Target Label': target_labels, 'Target Name': target_names})\n",
    "print(data_df.shape)\n",
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94050f6",
   "metadata": {},
   "source": [
    "From this dataset, we can see that each document has some textual content and the label can be denoted by a specific number, which maps to a newsgroup category name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c2d5728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty documents: 515\n"
     ]
    }
   ],
   "source": [
    "total_nulls = data_df[data_df.Article.str.strip() == ''].shape[0]\n",
    "print(\"Empty documents:\", total_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa200d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18331, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = data_df[~(data_df.Article.str.strip() == '')]\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5114ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Clean Article</th>\n",
       "      <th>Target Label</th>\n",
       "      <th>Target Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI am sure some bashers of Pens fans are pr...</td>\n",
       "      <td>sure bashers pen fan pretty confuse lack kind ...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My brother is in the market for a high-perform...</td>\n",
       "      <td>brother market high performance video card sup...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n\\n\\n\\n\\tFinally you said what you dream abou...</td>\n",
       "      <td>finally say what dream mediterranean new area ...</td>\n",
       "      <td>17</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nThink!\\n\\nIt's the SCSI card doing the DMA t...</td>\n",
       "      <td>think scsi card dma transfer disk scsi card dm...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1)    I have an old Jasmine drive which I cann...</td>\n",
       "      <td>1 old jasmine drive which cannot use new syste...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n\\nBack in high school I worked as a lab assi...</td>\n",
       "      <td>back high school work lab assistant bunch expe...</td>\n",
       "      <td>12</td>\n",
       "      <td>sci.electronics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\n\\nAE is in Dallas...try 214/241-6060 or 214/...</td>\n",
       "      <td>ae dallas try 214 241 6060 214 241 0055 tech s...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n[stuff deleted]\\n\\nOk, here's the solution t...</td>\n",
       "      <td>stuff delete ok solution problem move canada y...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\n\\n\\nYeah, it's the second one.  And I believ...</td>\n",
       "      <td>yeah second one believe price try get good loo...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\nIf a Christian means someone who believes in...</td>\n",
       "      <td>christian mean someone who believe divinity je...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  \\\n",
       "0  \\n\\nI am sure some bashers of Pens fans are pr...   \n",
       "1  My brother is in the market for a high-perform...   \n",
       "2  \\n\\n\\n\\n\\tFinally you said what you dream abou...   \n",
       "3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...   \n",
       "4  1)    I have an old Jasmine drive which I cann...   \n",
       "5  \\n\\nBack in high school I worked as a lab assi...   \n",
       "6  \\n\\nAE is in Dallas...try 214/241-6060 or 214/...   \n",
       "7  \\n[stuff deleted]\\n\\nOk, here's the solution t...   \n",
       "8  \\n\\n\\nYeah, it's the second one.  And I believ...   \n",
       "9  \\nIf a Christian means someone who believes in...   \n",
       "\n",
       "                                       Clean Article  Target Label  \\\n",
       "0  sure bashers pen fan pretty confuse lack kind ...            10   \n",
       "1  brother market high performance video card sup...             3   \n",
       "2  finally say what dream mediterranean new area ...            17   \n",
       "3  think scsi card dma transfer disk scsi card dm...             3   \n",
       "4  1 old jasmine drive which cannot use new syste...             4   \n",
       "5  back high school work lab assistant bunch expe...            12   \n",
       "6  ae dallas try 214 241 6060 214 241 0055 tech s...             4   \n",
       "7  stuff delete ok solution problem move canada y...            10   \n",
       "8  yeah second one believe price try get good loo...            10   \n",
       "9  christian mean someone who believe divinity je...            19   \n",
       "\n",
       "                Target Name  \n",
       "0          rec.sport.hockey  \n",
       "1  comp.sys.ibm.pc.hardware  \n",
       "2     talk.politics.mideast  \n",
       "3  comp.sys.ibm.pc.hardware  \n",
       "4     comp.sys.mac.hardware  \n",
       "5           sci.electronics  \n",
       "6     comp.sys.mac.hardware  \n",
       "7          rec.sport.hockey  \n",
       "8          rec.sport.hockey  \n",
       "9        talk.religion.misc  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import nltk\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "# normalize our corpus\n",
    "norm_corpus = preprocess(data_df['Article'], cleaning = True, stemming = False, stem_type = None, \n",
    "                         lemmatization = True, remove_stopwords = True)\n",
    "data_df['Clean Article'] = norm_corpus\n",
    "# view sample data\n",
    "data_df = data_df[['Article', 'Clean Article', 'Target Label', 'Target Name']]\n",
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2bdeb8",
   "metadata": {},
   "source": [
    "We now have a nice preprocessed and normalized corpus of articles. There might have been some documents that, after preprocessing, might end up being empty or null. We use the following code to test this assumption and remove these documents from our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "643a2f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18331 entries, 0 to 18845\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Article        18331 non-null  object\n",
      " 1   Clean Article  18306 non-null  object\n",
      " 2   Target Label   18331 non-null  int32 \n",
      " 3   Target Name    18331 non-null  object\n",
      "dtypes: int32(1), object(3)\n",
      "memory usage: 644.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data_df = data_df.replace(r'^(\\s?)+$', np.nan, regex=True)\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f980677",
   "metadata": {},
   "source": [
    "We definitely have some null articles after our preprocessing operation. We can safely remove these null documents using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b277620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18306 entries, 0 to 18305\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Article        18306 non-null  object\n",
      " 1   Clean Article  18306 non-null  object\n",
      " 2   Target Label   18306 non-null  int32 \n",
      " 3   Target Name    18306 non-null  object\n",
      "dtypes: int32(1), object(3)\n",
      "memory usage: 500.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data_df = data_df.dropna().reset_index(drop=True)\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86743f74",
   "metadata": {},
   "source": [
    "We can now use this dataset for building our text classification system. Feel free to store the dataset using the following code if needed so you don’t need to run the preprocessing step every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e322b555",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv(f'{RESULTS_PATH}/clean_newsgroups.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158f09ef",
   "metadata": {},
   "source": [
    "### Building Train and Test Datasets\n",
    "\n",
    "To build a machine learning system, we need to build our models on training data and then test and evaluate their performance on test data. Hence, we split our dataset into train and test datasets. We take a `train dataset, test dataset` split of 67%/33% of the total data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "091291a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12265,), (6041,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_corpus, test_corpus, train_label_nums, test_label_nums, train_label_names, test_label_names =\\\n",
    "                                 train_test_split(np.array(data_df['Clean Article']), np.array(data_df['Target Label']),\n",
    "                                                       np.array(data_df['Target Name']), test_size=0.33, random_state=42)\n",
    "\n",
    "train_corpus.shape, test_corpus.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7de7a7",
   "metadata": {},
   "source": [
    "You can also observe the distribution of the various articles by the different newsgroup categories using the \n",
    "following code. We can then get an idea of how many documents will be used to train the model and how many are used to test the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59b725aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target Label</th>\n",
       "      <th>Train Count</th>\n",
       "      <th>Test Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>669</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>664</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>660</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>651</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rec.autos</td>\n",
       "      <td>645</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>644</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>641</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>rec.motorcycles</td>\n",
       "      <td>640</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sci.electronics</td>\n",
       "      <td>639</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>misc.forsale</td>\n",
       "      <td>638</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "      <td>635</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>634</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sci.med</td>\n",
       "      <td>631</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>622</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>622</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>618</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>584</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>521</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>talk.politics.misc</td>\n",
       "      <td>496</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>411</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Target Label  Train Count  Test Count\n",
       "17                 sci.crypt          669         293\n",
       "5     soc.religion.christian          664         310\n",
       "15          rec.sport.hockey          660         313\n",
       "7              comp.graphics          651         302\n",
       "3                  rec.autos          645         290\n",
       "10            comp.windows.x          644         336\n",
       "12        rec.sport.baseball          641         314\n",
       "16           rec.motorcycles          640         329\n",
       "6            sci.electronics          639         317\n",
       "4               misc.forsale          638         321\n",
       "14   comp.os.ms-windows.misc          635         311\n",
       "13                 sci.space          634         320\n",
       "0                    sci.med          631         329\n",
       "8   comp.sys.ibm.pc.hardware          622         341\n",
       "11     talk.politics.mideast          622         295\n",
       "9      comp.sys.mac.hardware          618         309\n",
       "1         talk.politics.guns          584         301\n",
       "18               alt.atheism          521         258\n",
       "2         talk.politics.misc          496         258\n",
       "19        talk.religion.misc          411         194"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "trd = dict(Counter(train_label_names))\n",
    "tsd = dict(Counter(test_label_names))\n",
    "\n",
    "(pd.DataFrame([[key, trd[key], tsd[key]] for key in trd], \n",
    "    columns=['Target Label', 'Train Count', 'Test Count']).sort_values(by=['Train Count', 'Test Count'], ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558e0cf5",
   "metadata": {},
   "source": [
    "Above you can see the distribution of train and test articles by the 20 newsgroups. \n",
    "\n",
    "We now briefly cover the various feature engineering techniques, which we use in this chapter to build our text classification models.\n",
    "\n",
    "### Feature Engineering Techniques\n",
    "\n",
    "There are various feature extraction or feature engineering techniques. \n",
    "\n",
    "In a dataset, there are typically many data points, which are usually the rows of the dataset, and the columns are various features or properties of the dataset with specific values for each row or observation. In machine learning terminology, features are unique measurable attributes or properties for each observation or data point in a dataset. Features are usually numeric in nature and can be absolute numeric values or categorical features that can be encoded as binary features for each category in the list using a process called `one-hot` encoding . They can be represented as distinct numerical entities using a process called `label-encoding`. The process of extracting and selecting features is both an art and a science and this process is called `feature extraction` or `feature engineering`.\n",
    "\n",
    "Feature engineering is very important and is often known as the secret sauce to creating superior and better performing machine learning models. Extracted features are fed into machine learning algorithms for learning patterns that can be applied on future new data points for getting insights. These algorithms usually expect features in the form of numeric vectors because each algorithm is at heart a mathematical operation of optimization and minimizing loss and error when it tries to learn patterns from data points and observations. Hence, with textual data comes the added challenge of figuring out how to transform and extract numeric features from textual data.\n",
    "\n",
    "Traditional (count-based) feature engineering strategies for textual data involve models belonging to a family of models, popularly known as the Bag of Words model in general. While they are effective methods for extracting features from text, due to the inherent nature of the model being just a bag of unstructured words, we lose additional information like the semantics, structure, sequence, and context around nearby words in each text document.\n",
    "\n",
    "#### Bag of Words\n",
    "Use the `Bag of Words` model to represent each text document as a numeric vector where each dimension is a specific word from the corpus and the value could be its frequency in the document, occurrence (denoted by 1 or 0), or even weighted values. The model’s name is such because each document is represented literally as a bag of its own words, disregarding word orders, sequences, and grammar.\n",
    "\n",
    "Let’s start by using a basic Bag of Words, the term frequency-based feature engineering model, to extract features from our train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4e6a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# build BOW features on train articles\n",
    "cv = CountVectorizer(binary=False, min_df=0.0, max_df=1.0)\n",
    "cv_train_features = cv.fit_transform(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb8198ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform test articles into features\n",
    "cv_test_features = cv.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45d07819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW model:> Train features shape: (12265, 93559)  Test features shape: (6041, 93559)\n"
     ]
    }
   ],
   "source": [
    "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac24df4",
   "metadata": {},
   "source": [
    "#### Classification Models\n",
    "Classification models are supervised machine learning algorithms that are used to classify, categorize, or label data points based on what it has observed in the past.\n",
    "\n",
    "We now build several classifiers on these features using the training data and test their performance on the test dataset using all the classification models we discussed earlier. We also check model accuracies using five-fold cross validation just to see if the model performs consistently across the validation folds of data (we use this same strategy to tune the models later)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd131581",
   "metadata": {},
   "source": [
    "##### Multinomial NB \n",
    "This is a special case of the popular Naïve Bayes algorithm used specifically for prediction and classification tasks where we have more than two classes.\n",
    "\n",
    "The Naïve Bayes algorithm is a supervised learning algorithm that puts into action the very popular Bayes theorem. However there is a `naïve` assumption here that each feature is completely independent of the others. \n",
    "\n",
    "Multinomial Naïve Bayes is an extension of the NB algorithm for predicting and classifying data points, where the number of distinct classes or outcomes are more than two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c771772b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.66408479 0.65674684 0.6583775  0.65103954 0.65144721]\n",
      "Mean CV Accuracy: 0.6563391765185488\n",
      "Test Accuracy: 0.6661148816421122\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB(alpha=1)\n",
    "mnb.fit(cv_train_features, train_label_names)\n",
    "mnb_bow_cv_scores = cross_val_score(mnb, cv_train_features, train_label_names, cv=5)\n",
    "mnb_bow_cv_mean_score = np.mean(mnb_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', mnb_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', mnb_bow_cv_mean_score)\n",
    "mnb_bow_test_score = mnb.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', mnb_bow_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f623ae",
   "metadata": {},
   "source": [
    "##### Logistic Regression \n",
    "The logistic regression model is actually a statistical model developed by statistician David Cox in 1958. It is also known as the logit or logistic model since it uses the logistic (popularly also known as sigmoid) mathematical function to estimate the parameter values. These are the coefficients of all our features such that the overall loss is minimized when predicting the outcome—in this case, the newsgroup categories. However, we don’t focus on errors but more about maximizing the likelihood of the predicted values to the observed values using Maximum-Likelihood Estimation (MLE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0150736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.68772931 0.6742764  0.69629026 0.68854464 0.67549939]\n",
      "Mean CV Accuracy: 0.6844679983693437\n",
      "Test Accuracy: 0.6993875186227446\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', max_iter=100, C=1, random_state=42)\n",
    "lr.fit(cv_train_features, train_label_names)\n",
    "lr_bow_cv_scores = cross_val_score(lr, cv_train_features, train_label_names, cv=5)\n",
    "lr_bow_cv_mean_score = np.mean(lr_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', lr_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', lr_bow_cv_mean_score)\n",
    "lr_bow_test_score = lr.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', lr_bow_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd75c4d",
   "metadata": {},
   "source": [
    "##### Support Vector Machines \n",
    "In machine learning, support vector machines , known popularly as SVMs, are supervised learning algorithms. They are used for classification, regression, novelty and anomaly, and outlier detection. Considering a binary classification problem, if we have training data such that each data point or observation belongs to a specific class, the SVM algorithm can be trained based on this data such that it can assign future data points into one of the two classes. \n",
    "\n",
    "This algorithm represents the training data samples as points in space such that points belonging to either class can be separated by a wide gap between them (hyperplane) and the new data points to be predicted are assigned classes based on which side of this hyperplane they fall into. This process is for a typical linear classification process. However, SVM can also perform non-linear classification by an interesting approach known as a kernel trick, where kernel functions are used to operate on high-dimensional feature spaces that are non-linear separable. Usually, inner products between data points in the feature space help achieve this.\n",
    "\n",
    "The SVM algorithm takes in a set of training data points and constructs a hyperplane of a collection of hyperplanes for a high-dimensional feature space. The larger the margins of the hyperplane, the better the separation.\n",
    "\n",
    "The Scikit-Learn implementation of SVM can be found in SVC, LinearSVC, or SGDClassifier, where we use the hinge loss function (set by default) to optimize and build the model. This loss function helps us get the soft margins and is often known as a soft-margin SVM. You can also use different kernel functions to convert the existing feature space into an even higher dimensional feature space, where the data can be separated linearly. However, we do not recommend this a lot for text data problems since you already deal with a huge number of dimensions right from the start.\n",
    "\n",
    "For a multi-class classification problem, if we have `n` classes, for each class a binary classifier is trained and learned that helps is separating between each class and the other `n-1` classes. During prediction, the scores (distances to hyperplanes) for each classifier are computed and the maximum score is chosen for selecting the class label. The stochastic gradient descent is often used for minimizing the loss function in SVM algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c121fbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.64573991 0.64410925 0.65878516 0.63962495 0.64573991]\n",
      "Mean CV Accuracy: 0.6467998369343662\n",
      "Test Accuracy: 0.6586657838106273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm = LinearSVC(penalty='l2', C=1, random_state=42)\n",
    "svm.fit(cv_train_features, train_label_names)\n",
    "svm_bow_cv_scores = cross_val_score(svm, cv_train_features, train_label_names, cv=5)\n",
    "svm_bow_cv_mean_score = np.mean(svm_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svm_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', svm_bow_cv_mean_score)\n",
    "svm_bow_test_score = svm.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', svm_bow_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5aa8d3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.64084794 0.64777823 0.64573991 0.66163881 0.632287  ]\n",
      "Mean CV Accuracy: 0.6456583774969425\n",
      "Test Accuracy: 0.6383049164045688\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "svm_sgd = SGDClassifier(loss='hinge', penalty='l2', max_iter=5, random_state=42)\n",
    "svm_sgd.fit(cv_train_features, train_label_names)\n",
    "svmsgd_bow_cv_scores = cross_val_score(svm_sgd, cv_train_features, train_label_names, cv=5)\n",
    "svmsgd_bow_cv_mean_score = np.mean(svmsgd_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svmsgd_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', svmsgd_bow_cv_mean_score)\n",
    "svmsgd_bow_test_score = svm_sgd.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', svmsgd_bow_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3b60d6",
   "metadata": {},
   "source": [
    "##### Random Forest\n",
    "\n",
    "Decision trees are a family of supervised machine learning algorithms that can represent and interpret sets of rules automatically from the underlying data. They use metrics like information gain and gini-index to build the tree. However, a major drawback of decision trees is that since they are non-parametric, the more data there is, greater the depth of the tree. We can end up with really huge and deep trees that are prone to overfitting. The model might work really well on training data, but instead of learning, it just memorizes all the training samples and builds very specific rules to them. Hence, it performs really poorly on the test data. Random forests try to tackle this problem.\n",
    "\n",
    "A random forest is a meta-estimator or an ensemble model that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size, but the samples are drawn with replacement (bootstrap samples). In random forests, all the trees are trained in parallel (bagging model/bootstrap aggregation). Besides this, each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. Also, when splitting a node during the construction of the tree, the split that is chosen is no longer the best split among all features. Instead, the split that is picked is the best split among a random subset of the features. Thus the randomness introduced in a random forest is both due to random sampling of data and random selection of features when splitting nodes in each tree. Hence, due to this randomness, the bias of the forest usually slightly increases (with respect to the bias of a single non-random decision tree). However, due to averaging, the overall variance of the model decreases significantly as compared to the increase in bias and hence it gives us an overall better model.\n",
    "\n",
    "When building a random forest , you can set specific model parameters for both the base decision trees and the overall forest. For the trees, you usually have the same parameters as a normal decision tree model like the tree depth, number of leaves, number of features in each split, samples per leaf, criteria for the node splits, information gain, and gini impurity. For the forest, you can tune the total number of trees needed, the number of features to be used per tree, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "103219e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.50224215 0.51406441 0.52710966 0.52792499 0.51365675]\n",
      "Mean CV Accuracy: 0.5169995923359152\n",
      "Test Accuracy: 0.5158086409534846\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "rfc.fit(cv_train_features, train_label_names)\n",
    "rfc_bow_cv_scores = cross_val_score(rfc, cv_train_features, train_label_names, cv=5)\n",
    "rfc_bow_cv_mean_score = np.mean(rfc_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', rfc_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', rfc_bow_cv_mean_score)\n",
    "rfc_bow_test_score = rfc.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', rfc_bow_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e86e1be",
   "metadata": {},
   "source": [
    "##### Gradient boosting machines\n",
    "\n",
    "They, popularly known as Gradient Boosting Machines (GBMs), can be used for regression and classification. Typically, GBMs builds an additive model in a forward stage-wise sequential fashion; they allow for the optimization of arbitrary differentiable loss functions. GBMs can usually work on any combination of models (weak learners) and loss functions. Scikit-Learn uses GBRTs (Gradient Boosted Regression Trees), which are generalized boosting models that can be applied to arbitrary differentiable loss functions. The beauty of this model is that is accurate and can be used for both regression and classification problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "316cd916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.5413779  0.54545455 0.53607827 0.56257644 0.54178557]\n",
      "Mean CV Accuracy: 0.5454545454545454\n",
      "Test Accuracy: 0.5596755504055619\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=10, random_state=42)\n",
    "gbc.fit(cv_train_features, train_label_names)\n",
    "gbc_bow_cv_scores = cross_val_score(gbc, cv_train_features, train_label_names, cv=5)\n",
    "gbc_bow_cv_mean_score = np.mean(gbc_bow_cv_scores)\n",
    "print('CV Accuracy (5-fold):', gbc_bow_cv_scores)\n",
    "print('Mean CV Accuracy:', gbc_bow_cv_mean_score)\n",
    "gbc_bow_test_score = gbc.score(cv_test_features, test_label_names)\n",
    "print('Test Accuracy:', gbc_bow_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4538b9dc",
   "metadata": {},
   "source": [
    "It is interesting to see that simpler models like Naïve Bayes and Logistic Regression performed much better than the ensemble models. Let’s look at the next model pipeline now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d786f9b6",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "\n",
    "Use the `TF-IDF` model: TF-IDF stands for Term Frequency-Inverse Document Frequency and it’s a combination of two metrics, term frequency (TF) and inverse document frequency (IDF). This technique was originally developed as a metric for for showing search engine results based on user queries and has become part of information retrieval and text feature extraction.\n",
    "\n",
    "We use TF-IDF features to train our classification models. Assuming TF-IDF weighs down unimportant features, we might get better performing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a91bbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# build BOW features on train articles\n",
    "tv = TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0)\n",
    "tv_train_features = tv.fit_transform(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e95481ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform test articles into features\n",
    "tv_test_features = tv.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c098c800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF model:> Train features shape: (12265, 93559)  Test features shape: (6041, 93559)\n"
     ]
    }
   ],
   "source": [
    "print('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3bd9cb",
   "metadata": {},
   "source": [
    "#### Classification Models\n",
    "\n",
    "We now build several classifiers on these features using the training data and test their performance on the test dataset using all the classification models. We also check model accuracies using five-fold cross validation, just like we did earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3a4546c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.71259682 0.70158989 0.7264574  0.70118223 0.70729719]\n",
      "Mean CV Accuracy: 0.7098247044435386\n",
      "Test Accuracy: 0.713623572256249\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB(alpha=1)\n",
    "mnb.fit(tv_train_features, train_label_names)\n",
    "mnb_tfidf_cv_scores = cross_val_score(mnb, tv_train_features, train_label_names, cv=5)\n",
    "mnb_tfidf_cv_mean_score = np.mean(mnb_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', mnb_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', mnb_tfidf_cv_mean_score)\n",
    "mnb_tfidf_test_score = mnb.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', mnb_tfidf_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "689a2524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.7480636  0.73909499 0.74561761 0.74847126 0.74072564]\n",
      "Mean CV Accuracy: 0.7443946188340808\n",
      "Test Accuracy: 0.7531865585168018\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2', max_iter=100, C=1, random_state=42)\n",
    "lr.fit(tv_train_features, train_label_names)\n",
    "lr_tfidf_cv_scores = cross_val_score(lr, tv_train_features, train_label_names, cv=5)\n",
    "lr_tfidf_cv_mean_score = np.mean(lr_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', lr_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', lr_tfidf_cv_mean_score)\n",
    "lr_tfidf_test_score = lr.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', lr_tfidf_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3050b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.77048512 0.75703221 0.76477782 0.76559315 0.75947819]\n",
      "Mean CV Accuracy: 0.7634732980024459\n",
      "Test Accuracy: 0.7717265353418308\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(penalty='l2', C=1, random_state=42)\n",
    "svm.fit(tv_train_features, train_label_names)\n",
    "svm_tfidf_cv_scores = cross_val_score(svm, tv_train_features, train_label_names, cv=5)\n",
    "svm_tfidf_cv_mean_score = np.mean(svm_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svm_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', svm_tfidf_cv_mean_score)\n",
    "svm_tfidf_test_score = svm.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', svm_tfidf_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34a52ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.76151651 0.75580921 0.76029352 0.76314717 0.75662454]\n",
      "Mean CV Accuracy: 0.7594781899714635\n",
      "Test Accuracy: 0.7659327925840093\n"
     ]
    }
   ],
   "source": [
    "svm_sgd = SGDClassifier(loss='hinge', penalty='l2', max_iter=5, random_state=42)\n",
    "svm_sgd.fit(tv_train_features, train_label_names)\n",
    "svmsgd_tfidf_cv_scores = cross_val_score(svm_sgd, tv_train_features, train_label_names, cv=5)\n",
    "svmsgd_tfidf_cv_mean_score = np.mean(svmsgd_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svmsgd_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', svmsgd_tfidf_cv_mean_score)\n",
    "svmsgd_tfidf_test_score = svm_sgd.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', svmsgd_tfidf_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa785b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.52833265 0.50428047 0.50346514 0.52140236 0.51977171]\n",
      "Mean CV Accuracy: 0.5154504688136975\n",
      "Test Accuracy: 0.5302102300943552\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "rfc.fit(tv_train_features, train_label_names)\n",
    "rfc_tfidf_cv_scores = cross_val_score(rfc, tv_train_features, train_label_names, cv=5)\n",
    "rfc_tfidf_cv_mean_score = np.mean(rfc_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', rfc_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', rfc_tfidf_cv_mean_score)\n",
    "rfc_tfidf_test_score = rfc.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', rfc_tfidf_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c3f2670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy (5-fold): [0.5413779  0.54341623 0.53933958 0.5629841  0.54586221]\n",
      "Mean CV Accuracy: 0.5465960048919691\n",
      "Test Accuracy: 0.5573580533024334\n"
     ]
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=10, random_state=42)\n",
    "gbc.fit(tv_train_features, train_label_names)\n",
    "gbc_tfidf_cv_scores = cross_val_score(gbc, tv_train_features, train_label_names, cv=5)\n",
    "gbc_tfidf_cv_mean_score = np.mean(gbc_tfidf_cv_scores)\n",
    "print('CV Accuracy (5-fold):', gbc_tfidf_cv_scores)\n",
    "print('Mean CV Accuracy:', gbc_tfidf_cv_mean_score)\n",
    "gbc_tfidf_test_score = gbc.score(tv_test_features, test_label_names)\n",
    "print('Test Accuracy:', gbc_tfidf_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d243657",
   "metadata": {},
   "source": [
    "It’s interesting to see that the overall accuracy of several models increases by quite a bit, including logistic regression, Naïve Bayes, and SVM. Interestingly, the ensemble models don’t perform as well. Using more estimators might improve them, but still wouldn’t be as good as the other models and it would take a huge amount of training time.\n",
    "\n",
    "### Comparative Model Performance Evaluation\n",
    "\n",
    "We can now do a nice comparison of all the models we have tried so far with the two different feature engineering techniques. We will build a dataframe from our modeling results and compare the results.\n",
    "\n",
    "Combine score obtained with BoW and TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "939345d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>Linear SVM (SGD)</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Gradient Boosted Machines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CV Score (TF)</th>\n",
       "      <td>0.656339</td>\n",
       "      <td>0.684468</td>\n",
       "      <td>0.6468</td>\n",
       "      <td>0.645658</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Score (TF)</th>\n",
       "      <td>0.666115</td>\n",
       "      <td>0.699388</td>\n",
       "      <td>0.658666</td>\n",
       "      <td>0.638305</td>\n",
       "      <td>0.515809</td>\n",
       "      <td>0.559676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CV Score (TF-IDF)</th>\n",
       "      <td>0.709825</td>\n",
       "      <td>0.744395</td>\n",
       "      <td>0.763473</td>\n",
       "      <td>0.759478</td>\n",
       "      <td>0.51545</td>\n",
       "      <td>0.546596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test Score (TF-IDF)</th>\n",
       "      <td>0.713624</td>\n",
       "      <td>0.753187</td>\n",
       "      <td>0.771727</td>\n",
       "      <td>0.765933</td>\n",
       "      <td>0.53021</td>\n",
       "      <td>0.557358</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0                    1           2  \\\n",
       "Model                Naive Bayes  Logistic Regression  Linear SVM   \n",
       "CV Score (TF)           0.656339             0.684468      0.6468   \n",
       "Test Score (TF)         0.666115             0.699388    0.658666   \n",
       "CV Score (TF-IDF)       0.709825             0.744395    0.763473   \n",
       "Test Score (TF-IDF)     0.713624             0.753187    0.771727   \n",
       "\n",
       "                                    3              4  \\\n",
       "Model                Linear SVM (SGD)  Random Forest   \n",
       "CV Score (TF)                0.645658          0.517   \n",
       "Test Score (TF)              0.638305       0.515809   \n",
       "CV Score (TF-IDF)            0.759478        0.51545   \n",
       "Test Score (TF-IDF)          0.765933        0.53021   \n",
       "\n",
       "                                             5  \n",
       "Model                Gradient Boosted Machines  \n",
       "CV Score (TF)                         0.545455  \n",
       "Test Score (TF)                       0.559676  \n",
       "CV Score (TF-IDF)                     0.546596  \n",
       "Test Score (TF-IDF)                   0.557358  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([['Naive Bayes', mnb_bow_cv_mean_score, mnb_bow_test_score, \n",
    "               mnb_tfidf_cv_mean_score, mnb_tfidf_test_score],\n",
    "              ['Logistic Regression', lr_bow_cv_mean_score, lr_bow_test_score, \n",
    "               lr_tfidf_cv_mean_score, lr_tfidf_test_score],\n",
    "              ['Linear SVM', svm_bow_cv_mean_score, svm_bow_test_score, \n",
    "               svm_tfidf_cv_mean_score, svm_tfidf_test_score],\n",
    "              ['Linear SVM (SGD)', svmsgd_bow_cv_mean_score, svmsgd_bow_test_score, \n",
    "               svmsgd_tfidf_cv_mean_score, svmsgd_tfidf_test_score],\n",
    "              ['Random Forest', rfc_bow_cv_mean_score, rfc_bow_test_score, \n",
    "               rfc_tfidf_cv_mean_score, rfc_tfidf_test_score],\n",
    "              ['Gradient Boosted Machines', gbc_bow_cv_mean_score, gbc_bow_test_score, \n",
    "               gbc_tfidf_cv_mean_score, gbc_tfidf_test_score]],\n",
    "             columns=['Model', 'CV Score (TF)', 'Test Score (TF)', 'CV Score (TF-IDF)', 'Test Score (TF-IDF)'],\n",
    "             ).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d888c8",
   "metadata": {},
   "source": [
    "Result shows us that the best performing models were SVM followed by Logistic Regression and Naïve Bayes. Ensemble models did not perform as well on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a483d6f",
   "metadata": {},
   "source": [
    "### Model Tuning\n",
    "Model tuning is perhaps one of the key stages in the machine learning process and can lead to better performing models. Any machine learning model typically has hyperparameters, which are high-level concepts much like configuration settings that you can tune like knobs in a device! A very important point to remember is that hyperparameters are model parameters that are not directly learned within estimators and do not depend on the underlying data (as opposed to model parameters or coefficients like the coefficients of logistic regression, which can change based on the underlying training data).\n",
    "\n",
    "It is possible and recommended to search the hyperparameter space for the best cross-validation score for which we use a five-fold cross validation scheme along with grid search for finding the best hyperparameter values. A typical search for the best hyperparameter values during tuning consists of the following major components:\n",
    "* A model or estimator like LogisticRegression from Scikit-Learn\n",
    "* A hyperparameter space that we can define with values and ranges\n",
    "* A method for searching or sampling candidates like Grid Search\n",
    "* A cross-validation scheme, like five-fold cross-validation\n",
    "* A score function, like accuracy, for classification models\n",
    "\n",
    "There are two very common approaches for sampling search candidates also available in Scikit-Learn. We have GridSearchCV, which exhaustively considers all parameter combinations set by users. However, RandomizedSearchCV typically samples a given number of candidates from a parameter space with a specified distribution instead of taking all combinations. We use Grid Search for our tuning experiments.\n",
    "\n",
    "To tune the experiments, we also use a Scikit-Learn Pipeline object , which is an excellent way to chain multiple components together where we sequentially apply a list of transforms like data preprocessors, feature engineering methods, and a model estimator for predictions. Intermediate steps of the pipeline must be some form of a “transformer,” that is, they must implement fit and transform methods.\n",
    "\n",
    "The purpose of the pipeline and why we want to use it is so that we can assemble multiple components like feature \n",
    "engineering and modeling so that they can be cross-validated while setting different hyperparameter values for grid search. Let’s get started with tuning our Naïve Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2bc934b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] END ........mnb__alpha=1e-05, tfidf__ngram_range=(1, 1); total time=   2.6s\n",
      "[CV] END ........mnb__alpha=1e-05, tfidf__ngram_range=(1, 1); total time=   2.4s\n",
      "[CV] END ........mnb__alpha=1e-05, tfidf__ngram_range=(1, 1); total time=   2.7s\n",
      "[CV] END ........mnb__alpha=1e-05, tfidf__ngram_range=(1, 1); total time=   2.5s\n",
      "[CV] END ........mnb__alpha=1e-05, tfidf__ngram_range=(1, 1); total time=   2.6s\n",
      "[CV] END ........mnb__alpha=1e-05, tfidf__ngram_range=(1, 2); total time=  13.9s\n",
      "[CV] END ........mnb__alpha=1e-05, tfidf__ngram_range=(1, 2); total time=   9.8s\n",
      "[CV] END ........mnb__alpha=1e-05, tfidf__ngram_range=(1, 2); total time=  10.2s\n",
      "[CV] END ........mnb__alpha=1e-05, tfidf__ngram_range=(1, 2); total time=   9.9s\n",
      "[CV] END ........mnb__alpha=1e-05, tfidf__ngram_range=(1, 2); total time=  10.1s\n",
      "[CV] END .......mnb__alpha=0.0001, tfidf__ngram_range=(1, 1); total time=   2.4s\n",
      "[CV] END .......mnb__alpha=0.0001, tfidf__ngram_range=(1, 1); total time=   2.3s\n",
      "[CV] END .......mnb__alpha=0.0001, tfidf__ngram_range=(1, 1); total time=   3.0s\n",
      "[CV] END .......mnb__alpha=0.0001, tfidf__ngram_range=(1, 1); total time=   2.4s\n",
      "[CV] END .......mnb__alpha=0.0001, tfidf__ngram_range=(1, 1); total time=   2.5s\n",
      "[CV] END .......mnb__alpha=0.0001, tfidf__ngram_range=(1, 2); total time=  10.2s\n",
      "[CV] END .......mnb__alpha=0.0001, tfidf__ngram_range=(1, 2); total time=   9.9s\n",
      "[CV] END .......mnb__alpha=0.0001, tfidf__ngram_range=(1, 2); total time=  11.8s\n",
      "[CV] END .......mnb__alpha=0.0001, tfidf__ngram_range=(1, 2); total time=  10.4s\n",
      "[CV] END .......mnb__alpha=0.0001, tfidf__ngram_range=(1, 2); total time=  10.1s\n",
      "[CV] END .........mnb__alpha=0.01, tfidf__ngram_range=(1, 1); total time=   2.5s\n",
      "[CV] END .........mnb__alpha=0.01, tfidf__ngram_range=(1, 1); total time=   2.3s\n",
      "[CV] END .........mnb__alpha=0.01, tfidf__ngram_range=(1, 1); total time=   2.5s\n",
      "[CV] END .........mnb__alpha=0.01, tfidf__ngram_range=(1, 1); total time=   2.5s\n",
      "[CV] END .........mnb__alpha=0.01, tfidf__ngram_range=(1, 1); total time=   2.6s\n",
      "[CV] END .........mnb__alpha=0.01, tfidf__ngram_range=(1, 2); total time=  10.0s\n",
      "[CV] END .........mnb__alpha=0.01, tfidf__ngram_range=(1, 2); total time=   9.6s\n",
      "[CV] END .........mnb__alpha=0.01, tfidf__ngram_range=(1, 2); total time=  10.3s\n",
      "[CV] END .........mnb__alpha=0.01, tfidf__ngram_range=(1, 2); total time=  10.0s\n",
      "[CV] END .........mnb__alpha=0.01, tfidf__ngram_range=(1, 2); total time=  10.1s\n",
      "[CV] END ..........mnb__alpha=0.1, tfidf__ngram_range=(1, 1); total time=   4.1s\n",
      "[CV] END ..........mnb__alpha=0.1, tfidf__ngram_range=(1, 1); total time=   4.7s\n",
      "[CV] END ..........mnb__alpha=0.1, tfidf__ngram_range=(1, 1); total time=   3.2s\n",
      "[CV] END ..........mnb__alpha=0.1, tfidf__ngram_range=(1, 1); total time=   3.1s\n",
      "[CV] END ..........mnb__alpha=0.1, tfidf__ngram_range=(1, 1); total time=   2.5s\n",
      "[CV] END ..........mnb__alpha=0.1, tfidf__ngram_range=(1, 2); total time=  10.1s\n",
      "[CV] END ..........mnb__alpha=0.1, tfidf__ngram_range=(1, 2); total time=   9.9s\n",
      "[CV] END ..........mnb__alpha=0.1, tfidf__ngram_range=(1, 2); total time=  10.1s\n",
      "[CV] END ..........mnb__alpha=0.1, tfidf__ngram_range=(1, 2); total time=  10.0s\n",
      "[CV] END ..........mnb__alpha=0.1, tfidf__ngram_range=(1, 2); total time=  10.8s\n",
      "[CV] END ............mnb__alpha=1, tfidf__ngram_range=(1, 1); total time=   2.5s\n",
      "[CV] END ............mnb__alpha=1, tfidf__ngram_range=(1, 1); total time=   2.4s\n",
      "[CV] END ............mnb__alpha=1, tfidf__ngram_range=(1, 1); total time=   2.5s\n",
      "[CV] END ............mnb__alpha=1, tfidf__ngram_range=(1, 1); total time=   2.4s\n",
      "[CV] END ............mnb__alpha=1, tfidf__ngram_range=(1, 1); total time=   2.6s\n",
      "[CV] END ............mnb__alpha=1, tfidf__ngram_range=(1, 2); total time=   9.9s\n",
      "[CV] END ............mnb__alpha=1, tfidf__ngram_range=(1, 2); total time=   9.8s\n",
      "[CV] END ............mnb__alpha=1, tfidf__ngram_range=(1, 2); total time=  10.0s\n",
      "[CV] END ............mnb__alpha=1, tfidf__ngram_range=(1, 2); total time=   9.9s\n",
      "[CV] END ............mnb__alpha=1, tfidf__ngram_range=(1, 2); total time=  10.2s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "mnb_pipeline = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                        ('mnb', MultinomialNB())\n",
    "                       ])\n",
    "\n",
    "param_grid = {'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "              'mnb__alpha': [1e-5, 1e-4, 1e-2, 1e-1, 1]\n",
    "}\n",
    "\n",
    "gs_mnb = GridSearchCV(mnb_pipeline, param_grid, cv=5, verbose=2)\n",
    "gs_mnb = gs_mnb.fit(train_corpus, train_label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf499ab",
   "metadata": {},
   "source": [
    "We can now inspect the hyperparameter values chosen for our best estimator/model using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a980ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('tfidf', TfidfVectorizer(ngram_range=(1, 2))),\n",
       "  ('mnb', MultinomialNB(alpha=0.01))],\n",
       " 'verbose': False,\n",
       " 'tfidf': TfidfVectorizer(ngram_range=(1, 2)),\n",
       " 'mnb': MultinomialNB(alpha=0.01),\n",
       " 'tfidf__analyzer': 'word',\n",
       " 'tfidf__binary': False,\n",
       " 'tfidf__decode_error': 'strict',\n",
       " 'tfidf__dtype': numpy.float64,\n",
       " 'tfidf__encoding': 'utf-8',\n",
       " 'tfidf__input': 'content',\n",
       " 'tfidf__lowercase': True,\n",
       " 'tfidf__max_df': 1.0,\n",
       " 'tfidf__max_features': None,\n",
       " 'tfidf__min_df': 1,\n",
       " 'tfidf__ngram_range': (1, 2),\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__preprocessor': None,\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__stop_words': None,\n",
       " 'tfidf__strip_accents': None,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tfidf__tokenizer': None,\n",
       " 'tfidf__use_idf': True,\n",
       " 'tfidf__vocabulary': None,\n",
       " 'mnb__alpha': 0.01,\n",
       " 'mnb__class_prior': None,\n",
       " 'mnb__fit_prior': True}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_mnb.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46c3fbe",
   "metadata": {},
   "source": [
    "Now you might be wondering how these hyperparameters specifically were selected for the best estimator. Well, it decided this based on the model performance, with those hyperparameter values on the five-folds of validation data during cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d730ba96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>params</th>\n",
       "      <th>cv score (mean)</th>\n",
       "      <th>cv score (std)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>{'mnb__alpha': 0.01, 'tfidf__ngram_range': (1, 2)}</td>\n",
       "      <td>0.776519</td>\n",
       "      <td>0.004271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>{'mnb__alpha': 0.01, 'tfidf__ngram_range': (1, 1)}</td>\n",
       "      <td>0.774643</td>\n",
       "      <td>0.007864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>{'mnb__alpha': 0.0001, 'tfidf__ngram_range': (1, 2)}</td>\n",
       "      <td>0.760701</td>\n",
       "      <td>0.003554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>{'mnb__alpha': 0.1, 'tfidf__ngram_range': (1, 1)}</td>\n",
       "      <td>0.760049</td>\n",
       "      <td>0.007064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>{'mnb__alpha': 0.1, 'tfidf__ngram_range': (1, 2)}</td>\n",
       "      <td>0.759234</td>\n",
       "      <td>0.008669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>{'mnb__alpha': 1e-05, 'tfidf__ngram_range': (1, 2)}</td>\n",
       "      <td>0.751406</td>\n",
       "      <td>0.005198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>{'mnb__alpha': 0.0001, 'tfidf__ngram_range': (1, 1)}</td>\n",
       "      <td>0.745862</td>\n",
       "      <td>0.004674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>{'mnb__alpha': 1e-05, 'tfidf__ngram_range': (1, 1)}</td>\n",
       "      <td>0.733062</td>\n",
       "      <td>0.005361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>{'mnb__alpha': 1, 'tfidf__ngram_range': (1, 1)}</td>\n",
       "      <td>0.711374</td>\n",
       "      <td>0.008632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>{'mnb__alpha': 1, 'tfidf__ngram_range': (1, 2)}</td>\n",
       "      <td>0.706237</td>\n",
       "      <td>0.007194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank                                                params  \\\n",
       "5     1    {'mnb__alpha': 0.01, 'tfidf__ngram_range': (1, 2)}   \n",
       "4     2    {'mnb__alpha': 0.01, 'tfidf__ngram_range': (1, 1)}   \n",
       "3     3  {'mnb__alpha': 0.0001, 'tfidf__ngram_range': (1, 2)}   \n",
       "6     4     {'mnb__alpha': 0.1, 'tfidf__ngram_range': (1, 1)}   \n",
       "7     5     {'mnb__alpha': 0.1, 'tfidf__ngram_range': (1, 2)}   \n",
       "1     6   {'mnb__alpha': 1e-05, 'tfidf__ngram_range': (1, 2)}   \n",
       "2     7  {'mnb__alpha': 0.0001, 'tfidf__ngram_range': (1, 1)}   \n",
       "0     8   {'mnb__alpha': 1e-05, 'tfidf__ngram_range': (1, 1)}   \n",
       "8     9       {'mnb__alpha': 1, 'tfidf__ngram_range': (1, 1)}   \n",
       "9    10       {'mnb__alpha': 1, 'tfidf__ngram_range': (1, 2)}   \n",
       "\n",
       "   cv score (mean)  cv score (std)  \n",
       "5         0.776519        0.004271  \n",
       "4         0.774643        0.007864  \n",
       "3         0.760701        0.003554  \n",
       "6         0.760049        0.007064  \n",
       "7         0.759234        0.008669  \n",
       "1         0.751406        0.005198  \n",
       "2         0.745862        0.004674  \n",
       "0         0.733062        0.005361  \n",
       "8         0.711374        0.008632  \n",
       "9         0.706237        0.007194  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = gs_mnb.cv_results_\n",
    "results_df = pd.DataFrame({'rank': cv_results['rank_test_score'],\n",
    "                           'params': cv_results['params'], \n",
    "                           'cv score (mean)': cv_results['mean_test_score'], \n",
    "                           'cv score (std)': cv_results['std_test_score']} \n",
    "              )\n",
    "results_df = results_df.sort_values(by=['rank'], ascending=True)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea11c58b",
   "metadata": {},
   "source": [
    "Table shows model performances across different hyperparameter values in the hyperparameter space. You can see how the best hyperparameters including bi-gram TF-IDF features gave the best cross-validation accuracy. Note that we are never tuning \n",
    "our models based on test data scores, because that would end up biasing our model toward the test dataset. We can now check our tuned model’s performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98d30d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 0.7808309882469789\n"
     ]
    }
   ],
   "source": [
    "best_mnb_test_score = gs_mnb.score(test_corpus, test_label_names)\n",
    "print('Test Accuracy :', best_mnb_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b64ca2",
   "metadata": {},
   "source": [
    "Looks like we have achieved a model accuracy of 77.3%, which is an improvement of 6% over the base model! Let’s look at how it performs for logistic regression now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca865850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "711d9fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV] END .................lr__C=1, tfidf__ngram_range=(1, 1); total time= 1.1min\n",
      "[CV] END .................lr__C=1, tfidf__ngram_range=(1, 1); total time=  55.7s\n",
      "[CV] END .................lr__C=1, tfidf__ngram_range=(1, 1); total time=  58.5s\n",
      "[CV] END .................lr__C=1, tfidf__ngram_range=(1, 1); total time= 1.2min\n",
      "[CV] END .................lr__C=1, tfidf__ngram_range=(1, 1); total time=  58.4s\n",
      "[CV] END .................lr__C=1, tfidf__ngram_range=(1, 2); total time= 6.5min\n",
      "[CV] END .................lr__C=1, tfidf__ngram_range=(1, 2); total time= 6.3min\n",
      "[CV] END .................lr__C=1, tfidf__ngram_range=(1, 2); total time= 7.9min\n",
      "[CV] END .................lr__C=1, tfidf__ngram_range=(1, 2); total time= 7.4min\n",
      "[CV] END .................lr__C=1, tfidf__ngram_range=(1, 2); total time= 7.6min\n",
      "[CV] END .................lr__C=5, tfidf__ngram_range=(1, 1); total time=  48.6s\n",
      "[CV] END .................lr__C=5, tfidf__ngram_range=(1, 1); total time=  46.3s\n",
      "[CV] END .................lr__C=5, tfidf__ngram_range=(1, 1); total time=  52.7s\n",
      "[CV] END .................lr__C=5, tfidf__ngram_range=(1, 1); total time=  48.7s\n",
      "[CV] END .................lr__C=5, tfidf__ngram_range=(1, 1); total time=  49.5s\n",
      "[CV] END .................lr__C=5, tfidf__ngram_range=(1, 2); total time= 6.6min\n",
      "[CV] END .................lr__C=5, tfidf__ngram_range=(1, 2); total time= 6.9min\n",
      "[CV] END .................lr__C=5, tfidf__ngram_range=(1, 2); total time= 9.4min\n",
      "[CV] END .................lr__C=5, tfidf__ngram_range=(1, 2); total time= 8.8min\n",
      "[CV] END .................lr__C=5, tfidf__ngram_range=(1, 2); total time= 9.0min\n",
      "[CV] END ................lr__C=10, tfidf__ngram_range=(1, 1); total time= 1.1min\n",
      "[CV] END ................lr__C=10, tfidf__ngram_range=(1, 1); total time=  56.9s\n",
      "[CV] END ................lr__C=10, tfidf__ngram_range=(1, 1); total time= 1.3min\n",
      "[CV] END ................lr__C=10, tfidf__ngram_range=(1, 1); total time= 1.2min\n",
      "[CV] END ................lr__C=10, tfidf__ngram_range=(1, 1); total time= 1.2min\n",
      "[CV] END ................lr__C=10, tfidf__ngram_range=(1, 2); total time= 9.5min\n",
      "[CV] END ................lr__C=10, tfidf__ngram_range=(1, 2); total time= 8.7min\n",
      "[CV] END ................lr__C=10, tfidf__ngram_range=(1, 2); total time= 9.3min\n",
      "[CV] END ................lr__C=10, tfidf__ngram_range=(1, 2); total time= 8.9min\n",
      "[CV] END ................lr__C=10, tfidf__ngram_range=(1, 2); total time= 8.7min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lr_pipeline = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                        ('lr', LogisticRegression(penalty='l2', max_iter=100, random_state=42))\n",
    "                       ])\n",
    "\n",
    "param_grid = {'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "              'lr__C': [1, 5, 10]\n",
    "}\n",
    "\n",
    "gs_lr = GridSearchCV(lr_pipeline, param_grid, cv=5, verbose=2)\n",
    "gs_lr = gs_lr.fit(train_corpus, train_label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38cdc777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer(ngram_range=(1, 2))),\n",
       "                ('lr', LogisticRegression(C=10, random_state=42))])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "05af7032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 0.7705677867902665\n"
     ]
    }
   ],
   "source": [
    "best_lr_test_score = gs_lr.score(test_corpus, test_label_names)\n",
    "print('Test Accuracy :', best_lr_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e670e2",
   "metadata": {},
   "source": [
    "We get an overall test accuracy of approximately 77%, which is almost a 2.5% improvement from the base logistic regression model. Finally, let’s tune our top two SVM models—the regular Linear SVM model and the SVM with Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3caf9ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV] END .............svm__C=0.01, tfidf__ngram_range=(1, 1); total time=   2.5s\n",
      "[CV] END .............svm__C=0.01, tfidf__ngram_range=(1, 1); total time=   2.3s\n",
      "[CV] END .............svm__C=0.01, tfidf__ngram_range=(1, 1); total time=   2.4s\n",
      "[CV] END .............svm__C=0.01, tfidf__ngram_range=(1, 1); total time=   2.4s\n",
      "[CV] END .............svm__C=0.01, tfidf__ngram_range=(1, 1); total time=   2.4s\n",
      "[CV] END .............svm__C=0.01, tfidf__ngram_range=(1, 2); total time=   8.8s\n",
      "[CV] END .............svm__C=0.01, tfidf__ngram_range=(1, 2); total time=   8.7s\n",
      "[CV] END .............svm__C=0.01, tfidf__ngram_range=(1, 2); total time=   9.2s\n",
      "[CV] END .............svm__C=0.01, tfidf__ngram_range=(1, 2); total time=   8.9s\n",
      "[CV] END .............svm__C=0.01, tfidf__ngram_range=(1, 2); total time=   9.2s\n",
      "[CV] END ..............svm__C=0.1, tfidf__ngram_range=(1, 1); total time=   2.4s\n",
      "[CV] END ..............svm__C=0.1, tfidf__ngram_range=(1, 1); total time=   2.3s\n",
      "[CV] END ..............svm__C=0.1, tfidf__ngram_range=(1, 1); total time=   2.4s\n",
      "[CV] END ..............svm__C=0.1, tfidf__ngram_range=(1, 1); total time=   2.4s\n",
      "[CV] END ..............svm__C=0.1, tfidf__ngram_range=(1, 1); total time=   2.4s\n",
      "[CV] END ..............svm__C=0.1, tfidf__ngram_range=(1, 2); total time=  10.7s\n",
      "[CV] END ..............svm__C=0.1, tfidf__ngram_range=(1, 2); total time=   9.5s\n",
      "[CV] END ..............svm__C=0.1, tfidf__ngram_range=(1, 2); total time=  10.0s\n",
      "[CV] END ..............svm__C=0.1, tfidf__ngram_range=(1, 2); total time=   9.7s\n",
      "[CV] END ..............svm__C=0.1, tfidf__ngram_range=(1, 2); total time=  10.1s\n",
      "[CV] END ................svm__C=1, tfidf__ngram_range=(1, 1); total time=   2.8s\n",
      "[CV] END ................svm__C=1, tfidf__ngram_range=(1, 1); total time=   2.7s\n",
      "[CV] END ................svm__C=1, tfidf__ngram_range=(1, 1); total time=   2.9s\n",
      "[CV] END ................svm__C=1, tfidf__ngram_range=(1, 1); total time=   2.9s\n",
      "[CV] END ................svm__C=1, tfidf__ngram_range=(1, 1); total time=   2.9s\n",
      "[CV] END ................svm__C=1, tfidf__ngram_range=(1, 2); total time=  13.2s\n",
      "[CV] END ................svm__C=1, tfidf__ngram_range=(1, 2); total time=  13.0s\n",
      "[CV] END ................svm__C=1, tfidf__ngram_range=(1, 2); total time=  13.3s\n",
      "[CV] END ................svm__C=1, tfidf__ngram_range=(1, 2); total time=  13.4s\n",
      "[CV] END ................svm__C=1, tfidf__ngram_range=(1, 2); total time=  13.7s\n",
      "[CV] END ................svm__C=5, tfidf__ngram_range=(1, 1); total time=   4.7s\n",
      "[CV] END ................svm__C=5, tfidf__ngram_range=(1, 1); total time=   4.5s\n",
      "[CV] END ................svm__C=5, tfidf__ngram_range=(1, 1); total time=   4.6s\n",
      "[CV] END ................svm__C=5, tfidf__ngram_range=(1, 1); total time=   4.9s\n",
      "[CV] END ................svm__C=5, tfidf__ngram_range=(1, 1); total time=   5.0s\n",
      "[CV] END ................svm__C=5, tfidf__ngram_range=(1, 2); total time=  26.6s\n",
      "[CV] END ................svm__C=5, tfidf__ngram_range=(1, 2); total time=  27.5s\n",
      "[CV] END ................svm__C=5, tfidf__ngram_range=(1, 2); total time=  26.7s\n",
      "[CV] END ................svm__C=5, tfidf__ngram_range=(1, 2); total time=  28.2s\n",
      "[CV] END ................svm__C=5, tfidf__ngram_range=(1, 2); total time=  29.4s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svm_pipeline = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                        ('svm', LinearSVC(random_state=42))\n",
    "                       ])\n",
    "\n",
    "param_grid = {'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "              'svm__C': [0.01, 0.1, 1, 5]\n",
    "}\n",
    "\n",
    "gs_svm = GridSearchCV(svm_pipeline, param_grid, cv=5, verbose=2)\n",
    "gs_svm = gs_svm.fit(train_corpus, train_label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5965143d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('tfidf', TfidfVectorizer(ngram_range=(1, 2))),\n",
       "  ('svm', LinearSVC(C=5, random_state=42))],\n",
       " 'verbose': False,\n",
       " 'tfidf': TfidfVectorizer(ngram_range=(1, 2)),\n",
       " 'svm': LinearSVC(C=5, random_state=42),\n",
       " 'tfidf__analyzer': 'word',\n",
       " 'tfidf__binary': False,\n",
       " 'tfidf__decode_error': 'strict',\n",
       " 'tfidf__dtype': numpy.float64,\n",
       " 'tfidf__encoding': 'utf-8',\n",
       " 'tfidf__input': 'content',\n",
       " 'tfidf__lowercase': True,\n",
       " 'tfidf__max_df': 1.0,\n",
       " 'tfidf__max_features': None,\n",
       " 'tfidf__min_df': 1,\n",
       " 'tfidf__ngram_range': (1, 2),\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__preprocessor': None,\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__stop_words': None,\n",
       " 'tfidf__strip_accents': None,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tfidf__tokenizer': None,\n",
       " 'tfidf__use_idf': True,\n",
       " 'tfidf__vocabulary': None,\n",
       " 'svm__C': 5,\n",
       " 'svm__class_weight': None,\n",
       " 'svm__dual': True,\n",
       " 'svm__fit_intercept': True,\n",
       " 'svm__intercept_scaling': 1,\n",
       " 'svm__loss': 'squared_hinge',\n",
       " 'svm__max_iter': 1000,\n",
       " 'svm__multi_class': 'ovr',\n",
       " 'svm__penalty': 'l2',\n",
       " 'svm__random_state': 42,\n",
       " 'svm__tol': 0.0001,\n",
       " 'svm__verbose': 0}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_svm.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "152916c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 0.7867902665121669\n"
     ]
    }
   ],
   "source": [
    "best_svm_test_score = gs_svm.score(test_corpus, test_label_names)\n",
    "print('Test Accuracy :', best_svm_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "67855daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV] END ........sgd__alpha=1e-07, tfidf__ngram_range=(1, 1); total time=   2.5s\n",
      "[CV] END ........sgd__alpha=1e-07, tfidf__ngram_range=(1, 1); total time=   2.4s\n",
      "[CV] END ........sgd__alpha=1e-07, tfidf__ngram_range=(1, 1); total time=   2.6s\n",
      "[CV] END ........sgd__alpha=1e-07, tfidf__ngram_range=(1, 1); total time=   2.4s\n",
      "[CV] END ........sgd__alpha=1e-07, tfidf__ngram_range=(1, 1); total time=   2.5s\n",
      "[CV] END ........sgd__alpha=1e-07, tfidf__ngram_range=(1, 2); total time=   9.1s\n",
      "[CV] END ........sgd__alpha=1e-07, tfidf__ngram_range=(1, 2); total time=   9.0s\n",
      "[CV] END ........sgd__alpha=1e-07, tfidf__ngram_range=(1, 2); total time=   9.2s\n",
      "[CV] END ........sgd__alpha=1e-07, tfidf__ngram_range=(1, 2); total time=   9.1s\n",
      "[CV] END ........sgd__alpha=1e-07, tfidf__ngram_range=(1, 2); total time=   9.3s\n",
      "[CV] END ........sgd__alpha=1e-06, tfidf__ngram_range=(1, 1); total time=   2.4s\n",
      "[CV] END ........sgd__alpha=1e-06, tfidf__ngram_range=(1, 1); total time=   2.3s\n",
      "[CV] END ........sgd__alpha=1e-06, tfidf__ngram_range=(1, 1); total time=   2.4s\n",
      "[CV] END ........sgd__alpha=1e-06, tfidf__ngram_range=(1, 1); total time=   2.4s\n",
      "[CV] END ........sgd__alpha=1e-06, tfidf__ngram_range=(1, 1); total time=   2.4s\n",
      "[CV] END ........sgd__alpha=1e-06, tfidf__ngram_range=(1, 2); total time=   8.8s\n",
      "[CV] END ........sgd__alpha=1e-06, tfidf__ngram_range=(1, 2); total time=   8.4s\n",
      "[CV] END ........sgd__alpha=1e-06, tfidf__ngram_range=(1, 2); total time=   8.8s\n",
      "[CV] END ........sgd__alpha=1e-06, tfidf__ngram_range=(1, 2); total time=   8.7s\n",
      "[CV] END ........sgd__alpha=1e-06, tfidf__ngram_range=(1, 2); total time=   8.9s\n",
      "[CV] END ........sgd__alpha=1e-05, tfidf__ngram_range=(1, 1); total time=   2.4s\n",
      "[CV] END ........sgd__alpha=1e-05, tfidf__ngram_range=(1, 1); total time=   2.3s\n",
      "[CV] END ........sgd__alpha=1e-05, tfidf__ngram_range=(1, 1); total time=   2.3s\n",
      "[CV] END ........sgd__alpha=1e-05, tfidf__ngram_range=(1, 1); total time=   2.4s\n",
      "[CV] END ........sgd__alpha=1e-05, tfidf__ngram_range=(1, 1); total time=   2.3s\n",
      "[CV] END ........sgd__alpha=1e-05, tfidf__ngram_range=(1, 2); total time=   8.5s\n",
      "[CV] END ........sgd__alpha=1e-05, tfidf__ngram_range=(1, 2); total time=   8.2s\n",
      "[CV] END ........sgd__alpha=1e-05, tfidf__ngram_range=(1, 2); total time=   8.6s\n",
      "[CV] END ........sgd__alpha=1e-05, tfidf__ngram_range=(1, 2); total time=   8.3s\n",
      "[CV] END ........sgd__alpha=1e-05, tfidf__ngram_range=(1, 2); total time=   8.7s\n",
      "[CV] END .......sgd__alpha=0.0001, tfidf__ngram_range=(1, 1); total time=   2.4s\n",
      "[CV] END .......sgd__alpha=0.0001, tfidf__ngram_range=(1, 1); total time=   2.3s\n",
      "[CV] END .......sgd__alpha=0.0001, tfidf__ngram_range=(1, 1); total time=   2.4s\n",
      "[CV] END .......sgd__alpha=0.0001, tfidf__ngram_range=(1, 1); total time=   2.4s\n",
      "[CV] END .......sgd__alpha=0.0001, tfidf__ngram_range=(1, 1); total time=   2.4s\n",
      "[CV] END .......sgd__alpha=0.0001, tfidf__ngram_range=(1, 2); total time=   9.8s\n",
      "[CV] END .......sgd__alpha=0.0001, tfidf__ngram_range=(1, 2); total time=   9.4s\n",
      "[CV] END .......sgd__alpha=0.0001, tfidf__ngram_range=(1, 2); total time=   9.8s\n",
      "[CV] END .......sgd__alpha=0.0001, tfidf__ngram_range=(1, 2); total time=   9.5s\n",
      "[CV] END .......sgd__alpha=0.0001, tfidf__ngram_range=(1, 2); total time=   9.6s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "sgd_pipeline = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                        ('sgd', SGDClassifier(random_state=42))\n",
    "                       ])\n",
    "\n",
    "param_grid = {'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "              'sgd__alpha': [1e-7, 1e-6, 1e-5, 1e-4]\n",
    "}\n",
    "\n",
    "gs_sgd = GridSearchCV(sgd_pipeline, param_grid, cv=5, verbose=2)\n",
    "gs_sgd = gs_sgd.fit(train_corpus, train_label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e88f5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('tfidf', TfidfVectorizer(ngram_range=(1, 2))),\n",
       "  ('sgd', SGDClassifier(alpha=1e-05, random_state=42))],\n",
       " 'verbose': False,\n",
       " 'tfidf': TfidfVectorizer(ngram_range=(1, 2)),\n",
       " 'sgd': SGDClassifier(alpha=1e-05, random_state=42),\n",
       " 'tfidf__analyzer': 'word',\n",
       " 'tfidf__binary': False,\n",
       " 'tfidf__decode_error': 'strict',\n",
       " 'tfidf__dtype': numpy.float64,\n",
       " 'tfidf__encoding': 'utf-8',\n",
       " 'tfidf__input': 'content',\n",
       " 'tfidf__lowercase': True,\n",
       " 'tfidf__max_df': 1.0,\n",
       " 'tfidf__max_features': None,\n",
       " 'tfidf__min_df': 1,\n",
       " 'tfidf__ngram_range': (1, 2),\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__preprocessor': None,\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__stop_words': None,\n",
       " 'tfidf__strip_accents': None,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tfidf__tokenizer': None,\n",
       " 'tfidf__use_idf': True,\n",
       " 'tfidf__vocabulary': None,\n",
       " 'sgd__alpha': 1e-05,\n",
       " 'sgd__average': False,\n",
       " 'sgd__class_weight': None,\n",
       " 'sgd__early_stopping': False,\n",
       " 'sgd__epsilon': 0.1,\n",
       " 'sgd__eta0': 0.0,\n",
       " 'sgd__fit_intercept': True,\n",
       " 'sgd__l1_ratio': 0.15,\n",
       " 'sgd__learning_rate': 'optimal',\n",
       " 'sgd__loss': 'hinge',\n",
       " 'sgd__max_iter': 1000,\n",
       " 'sgd__n_iter_no_change': 5,\n",
       " 'sgd__n_jobs': None,\n",
       " 'sgd__penalty': 'l2',\n",
       " 'sgd__power_t': 0.5,\n",
       " 'sgd__random_state': 42,\n",
       " 'sgd__shuffle': True,\n",
       " 'sgd__tol': 0.001,\n",
       " 'sgd__validation_fraction': 0.1,\n",
       " 'sgd__verbose': 0,\n",
       " 'sgd__warm_start': False}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_sgd.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aec16c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 0.7841416983943056\n"
     ]
    }
   ],
   "source": [
    "best_sgd_test_score = gs_sgd.score(test_corpus, test_label_names)\n",
    "print('Test Accuracy :', best_sgd_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1655a",
   "metadata": {},
   "source": [
    "This is definitely the highest overall accuracy we have obtained so far! However, not a huge improvement from the default linear SVM model performance. The SVM with SGD gives us a tuned model accuracy of 76.8%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a65ddf",
   "metadata": {},
   "source": [
    "### Compute performance metrics \n",
    "\n",
    "Choosing the best model for deployment depends on a number of factors, like the model speed, accuracy, ease of use, understanding, and so on. Based on all the models we have built, the Naïve Bayes model is the fastest to train and, even though the SVM model might be slightly better on the test dataset in terms of accuracy, SVMs are notoriously slow and often hard to scale. Let’s take a detailed performance evaluation of our best, tuned Naïve Bayes model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "08dbce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_predictions = gs_mnb.predict(test_corpus)\n",
    "unique_classes = list(set(test_label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1f8c0bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7808\n",
      "Precision: 0.7884\n",
      "Recall: 0.7808\n",
      "F1 Score: 0.7769\n"
     ]
    }
   ],
   "source": [
    "import model_evaluation_utils as meu\n",
    "meu.get_metrics(true_labels=test_label_names, predicted_labels=mnb_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ce1e04",
   "metadata": {},
   "source": [
    "It is good to see good consistency with the classification metrics. Besides seeing the holistic view of model performance metrics, often a more granular view into per-class model performance metrics helps. Let’s take a look at that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0f54a384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "            misc.forsale       0.81      0.79      0.80       321\n",
      "      talk.politics.misc       0.76      0.62      0.68       258\n",
      "           comp.graphics       0.66      0.76      0.71       302\n",
      "             alt.atheism       0.75      0.57      0.65       258\n",
      "         rec.motorcycles       0.86      0.78      0.82       329\n",
      "               sci.crypt       0.77      0.86      0.82       293\n",
      "               sci.space       0.80      0.82      0.81       320\n",
      "      talk.religion.misc       0.79      0.28      0.42       194\n",
      "          comp.windows.x       0.84      0.83      0.84       336\n",
      " comp.os.ms-windows.misc       0.78      0.71      0.74       311\n",
      "      rec.sport.baseball       0.94      0.87      0.90       314\n",
      "   comp.sys.mac.hardware       0.81      0.75      0.78       309\n",
      "comp.sys.ibm.pc.hardware       0.72      0.75      0.73       341\n",
      "      talk.politics.guns       0.70      0.79      0.74       301\n",
      "  soc.religion.christian       0.60      0.94      0.73       310\n",
      "        rec.sport.hockey       0.89      0.95      0.92       313\n",
      "         sci.electronics       0.81      0.72      0.76       317\n",
      "               rec.autos       0.77      0.82      0.80       290\n",
      "                 sci.med       0.88      0.85      0.86       329\n",
      "   talk.politics.mideast       0.79      0.92      0.85       295\n",
      "\n",
      "                accuracy                           0.78      6041\n",
      "               macro avg       0.79      0.77      0.77      6041\n",
      "            weighted avg       0.79      0.78      0.78      6041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "meu.display_classification_report(true_labels=test_label_names, \n",
    "                                  predicted_labels=mnb_predictions, classes=unique_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f059e459",
   "metadata": {},
   "source": [
    "This gives us a nice overview into the model performance for each newsgroup class and interestingly some categories like religion, Christianity, and atheism have slightly lower performance. Could it be that the model is getting some of these mixed up? The confusion matrix is a great way to test this assumption. Let’s first look at the newsgroup name to number mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "264c9f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label Name</th>\n",
       "      <th>Label Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>misc.forsale</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rec.autos</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rec.motorcycles</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rec.sport.baseball</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sci.electronics</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>sci.med</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sci.space</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>talk.politics.misc</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Label Name  Label Number\n",
       "0                alt.atheism             0\n",
       "1              comp.graphics             1\n",
       "2    comp.os.ms-windows.misc             2\n",
       "3   comp.sys.ibm.pc.hardware             3\n",
       "4      comp.sys.mac.hardware             4\n",
       "5             comp.windows.x             5\n",
       "6               misc.forsale             6\n",
       "7                  rec.autos             7\n",
       "8            rec.motorcycles             8\n",
       "9         rec.sport.baseball             9\n",
       "10          rec.sport.hockey            10\n",
       "11                 sci.crypt            11\n",
       "12           sci.electronics            12\n",
       "13                   sci.med            13\n",
       "14                 sci.space            14\n",
       "15    soc.religion.christian            15\n",
       "16        talk.politics.guns            16\n",
       "17     talk.politics.mideast            17\n",
       "18        talk.politics.misc            18\n",
       "19        talk.religion.misc            19"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_data_map = {v:k for k, v in data_labels_map.items()}\n",
    "label_map_df = pd.DataFrame(list(label_data_map.items()), columns=['Label Name', 'Label Number'])\n",
    "label_map_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8370d163",
   "metadata": {},
   "source": [
    "We could now build a confusion matrix to show the correct and misclassified instances of each class label, which we represent by numbers for display purposes, due to the long names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3da6312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_class_nums = label_map_df['Label Number'].values\n",
    "mnb_prediction_class_nums = [label_data_map[item] for item in mnb_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "11395578",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_classes = label_map_df['Label Name'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f390e41",
   "metadata": {},
   "source": [
    "Let’s take a closer look at these class labels to see what their newsgroup names are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d63bc5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label Name</th>\n",
       "      <th>Label Number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Label Name  Label Number\n",
       "0              alt.atheism             0\n",
       "15  soc.religion.christian            15\n",
       "19      talk.religion.misc            19"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map_df[label_map_df['Label Number'].isin([0, 15, 19])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ac7872",
   "metadata": {},
   "source": [
    "All the newsgroup pertaining to different aspects of region have more misclassifications. Let's explore some specific instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2c6d22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9877, 12655,  7048, ...,  7624,  1594,  6293])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx, test_idx = train_test_split(np.array(range(len(data_df['Article']))), test_size=0.33, random_state=42)\n",
    "test_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09739770",
   "metadata": {},
   "source": [
    "Let's add two columns to our dataframe in our test dataset. The first column is the predictied label from our Naive Bayes model and the second column is the confidence of the model when making the prediction, which is basically the probability of the model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f1abb706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Clean Article</th>\n",
       "      <th>Target Label</th>\n",
       "      <th>Target Name</th>\n",
       "      <th>Predicted Name</th>\n",
       "      <th>Predicted Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9877</th>\n",
       "      <td>\\nCENTERS\\n[...]\\n[...]\\n\\nSanderson will be on Team Canada, but he'd be out of position as a ce...</td>\n",
       "      <td>center sanderson team canada position center although draft center play rookie sanderson score 4...</td>\n",
       "      <td>10</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>rec.sport.hockey</td>\n",
       "      <td>0.997253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12655</th>\n",
       "      <td>\\nI do not think they can use the eavesdropping as evidence at all. However,\\nusing the info the...</td>\n",
       "      <td>think use eavesdrop evidence however use info gather listen go search right place find good stro...</td>\n",
       "      <td>11</td>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>0.999950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7048</th>\n",
       "      <td>\\nThere are a few bills not yet in the archive, but these are the main ones\\nwe need to fight.  ...</td>\n",
       "      <td>bill yet archive main ones need fight thank david robinson scan many us subdirectory bill store ...</td>\n",
       "      <td>16</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "      <td>sci.crypt</td>\n",
       "      <td>0.342186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15056</th>\n",
       "      <td>\\n\\nUnfortunately there a *LOT* of such software.  I also find it to be\\nthe case that the major...</td>\n",
       "      <td>unfortunately lot software also find case majority software bad regard commercial software way m...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "      <td>comp.os.ms-windows.misc</td>\n",
       "      <td>0.388951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17485</th>\n",
       "      <td>I have looked through the FAQ sections and have not\\nseen a answer for this.\\n\\nI have an X/Moti...</td>\n",
       "      <td>look faq section see answer x motif application write couple gif file pict scan color scanner wo...</td>\n",
       "      <td>5</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>0.885876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                   Article  \\\n",
       "9877   \\nCENTERS\\n[...]\\n[...]\\n\\nSanderson will be on Team Canada, but he'd be out of position as a ce...   \n",
       "12655  \\nI do not think they can use the eavesdropping as evidence at all. However,\\nusing the info the...   \n",
       "7048   \\nThere are a few bills not yet in the archive, but these are the main ones\\nwe need to fight.  ...   \n",
       "15056  \\n\\nUnfortunately there a *LOT* of such software.  I also find it to be\\nthe case that the major...   \n",
       "17485  I have looked through the FAQ sections and have not\\nseen a answer for this.\\n\\nI have an X/Moti...   \n",
       "\n",
       "                                                                                             Clean Article  \\\n",
       "9877   center sanderson team canada position center although draft center play rookie sanderson score 4...   \n",
       "12655  think use eavesdrop evidence however use info gather listen go search right place find good stro...   \n",
       "7048   bill yet archive main ones need fight thank david robinson scan many us subdirectory bill store ...   \n",
       "15056  unfortunately lot software also find case majority software bad regard commercial software way m...   \n",
       "17485  look faq section see answer x motif application write couple gif file pict scan color scanner wo...   \n",
       "\n",
       "       Target Label               Target Name           Predicted Name  \\\n",
       "9877             10          rec.sport.hockey         rec.sport.hockey   \n",
       "12655            11                 sci.crypt                sci.crypt   \n",
       "7048             16        talk.politics.guns                sci.crypt   \n",
       "15056             3  comp.sys.ibm.pc.hardware  comp.os.ms-windows.misc   \n",
       "17485             5            comp.windows.x            comp.graphics   \n",
       "\n",
       "       Predicted Confidence  \n",
       "9877               0.997253  \n",
       "12655              0.999950  \n",
       "7048               0.342186  \n",
       "15056              0.388951  \n",
       "17485              0.885876  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_probas = gs_mnb.predict_proba(test_corpus).max(axis=1)\n",
    "test_df = data_df.iloc[test_idx]\n",
    "test_df['Predicted Name'] = mnb_predictions\n",
    "test_df['Predicted Confidence'] = predict_probas\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3446ee",
   "metadata": {},
   "source": [
    "Let's now take a look at some articles that were from the newsgroup `talk.religion.misc`, but our model predicted `soc.religion.christian` with the highest confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3a0fdc30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Clean Article</th>\n",
       "      <th>Target Label</th>\n",
       "      <th>Target Name</th>\n",
       "      <th>Predicted Name</th>\n",
       "      <th>Predicted Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6237</th>\n",
       "      <td>Brian Ceccarelli wrote (that's me):\\n\\n\\nKent Sandvik responds:\\n\\n\\nI think I see where you are coming from Kent.  Jesus doesn't view\\nguilt like our modern venacular colors it.   \\n\\n\"Feelings\" ...</td>\n",
       "      <td>brian ceccarelli write kent sandvik respond think see where come kent jesus view guilt like modern venacular color feel nothing guilt feel arise state guilty feel guilt mutally exclusive feel reac...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>0.999710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13307</th>\n",
       "      <td>THE DIVINE MASTERS       \\n      \\n               Most Christians would agree, and correctly so, that \\n          Jesus Christ was a Divine Master, and a projection of God \\n          into the phy...</td>\n",
       "      <td>divine master christians would agree correctly jesus christ divine master projection god physical world god incarnate important relate facts christians completely ignorant followers world religion...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>0.999601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17282</th>\n",
       "      <td>The primary problem in human nature is a \"fragmentation of being.\"\\nHumans are in a state of tension, a tension of opposites. Good and\\nevil are the most thought provoking polarities that come to ...</td>\n",
       "      <td>primary problem human nature fragmentation humans state tension tension opposites good evil think provoke polarities come mind bible provide us many examples fragmentation war opposites within us ...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>0.999427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4367</th>\n",
       "      <td>:\\n (lots of stuff about the Nicene Creed deleted which can be read in the\\n  original basenote.  I will also leave it up to other LDS netters to\\n  take Mr. Weiss to task on using Mormon Doctrine...</td>\n",
       "      <td>lot stuff nicene creed delete which read original basenote also leave lds netters take mr weiss task use mormon doctrine declare difinitive word what lds church teach doctrine hopefully lds netter...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>0.999261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18218</th>\n",
       "      <td>\\n\\nJesus also recognized other holy days, like the Passover.  Acts 15 says \\nthat no more should be layed on the Gentiles than that which is necessary.\\nThe sabbath is not in the list, nor do any...</td>\n",
       "      <td>jesus also recognize holy days like passover act 15 say lay gentiles which necessary sabbath list epistles instruct people keep 7th day christians live among people who keep 7th day look like woul...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>0.998448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                       Article  \\\n",
       "6237   Brian Ceccarelli wrote (that's me):\\n\\n\\nKent Sandvik responds:\\n\\n\\nI think I see where you are coming from Kent.  Jesus doesn't view\\nguilt like our modern venacular colors it.   \\n\\n\"Feelings\" ...   \n",
       "13307  THE DIVINE MASTERS       \\n      \\n               Most Christians would agree, and correctly so, that \\n          Jesus Christ was a Divine Master, and a projection of God \\n          into the phy...   \n",
       "17282  The primary problem in human nature is a \"fragmentation of being.\"\\nHumans are in a state of tension, a tension of opposites. Good and\\nevil are the most thought provoking polarities that come to ...   \n",
       "4367   :\\n (lots of stuff about the Nicene Creed deleted which can be read in the\\n  original basenote.  I will also leave it up to other LDS netters to\\n  take Mr. Weiss to task on using Mormon Doctrine...   \n",
       "18218  \\n\\nJesus also recognized other holy days, like the Passover.  Acts 15 says \\nthat no more should be layed on the Gentiles than that which is necessary.\\nThe sabbath is not in the list, nor do any...   \n",
       "\n",
       "                                                                                                                                                                                                 Clean Article  \\\n",
       "6237   brian ceccarelli write kent sandvik respond think see where come kent jesus view guilt like modern venacular color feel nothing guilt feel arise state guilty feel guilt mutally exclusive feel reac...   \n",
       "13307  divine master christians would agree correctly jesus christ divine master projection god physical world god incarnate important relate facts christians completely ignorant followers world religion...   \n",
       "17282  primary problem human nature fragmentation humans state tension tension opposites good evil think provoke polarities come mind bible provide us many examples fragmentation war opposites within us ...   \n",
       "4367   lot stuff nicene creed delete which read original basenote also leave lds netters take mr weiss task use mormon doctrine declare difinitive word what lds church teach doctrine hopefully lds netter...   \n",
       "18218  jesus also recognize holy days like passover act 15 say lay gentiles which necessary sabbath list epistles instruct people keep 7th day christians live among people who keep 7th day look like woul...   \n",
       "\n",
       "       Target Label         Target Name          Predicted Name  \\\n",
       "6237             19  talk.religion.misc  soc.religion.christian   \n",
       "13307            19  talk.religion.misc  soc.religion.christian   \n",
       "17282            19  talk.religion.misc  soc.religion.christian   \n",
       "4367             19  talk.religion.misc  soc.religion.christian   \n",
       "18218            19  talk.religion.misc  soc.religion.christian   \n",
       "\n",
       "       Predicted Confidence  \n",
       "6237               0.999710  \n",
       "13307              0.999601  \n",
       "17282              0.999427  \n",
       "4367               0.999261  \n",
       "18218              0.998448  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "res_df = (test_df[(test_df['Target Name'] == 'talk.religion.misc') & (test_df['Predicted Name'] == 'soc.religion.christian')]\n",
    "       .sort_values(by=['Predicted Confidence'], ascending=False).head(5))\n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d4da6b",
   "metadata": {},
   "source": [
    "You can have an idea about which instances might be getting misclassified and why., It looks like there are definitely some aspects of Christianity also mentioned in some of these articles, which leads the model to predict the `soc.religion.christian` category.\n",
    "\n",
    "Let's now take a look at some articles that were from the newsgroup `talk.religion.misc`, but our model predicted `alt.atheism` with the highest confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e862d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Clean Article</th>\n",
       "      <th>Target Label</th>\n",
       "      <th>Target Name</th>\n",
       "      <th>Predicted Name</th>\n",
       "      <th>Predicted Confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>Why is the NT tossed out as info on Jesus.  I realize it is normally tossed\\nout because it contains miracles, but what are the other reasons?\\n\\nMAC\\n--\\n*****************************************...</td>\n",
       "      <td>why nt toss info jesus realize normally toss contain miracles what reason mac michael cobb raise tax middle university illinois class pay program champaign urbana bill clinton 3rd debate cobb alex...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>0.999882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5746</th>\n",
       "      <td>: In my mind, to say that science has its basis in values is a bit of a\\n: reach. Science has its basis in observable fact. \\n\\nI'd say that what one chooses to observe and how the observation is\\...</td>\n",
       "      <td>mind say science basis value bite reach science basis observable fact say what one choose observe how observation interpret what significance give depend great deal value observer science human ac...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>0.992743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11119</th>\n",
       "      <td>\\n\\n\\tUnless God admits that he didn't do it....\\n\\n\\t=)\\n\\n\\n---  \\n\\n  \" I'd Cheat on Hillary Too.\"</td>\n",
       "      <td>unless god admit cheat hillary</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>0.893857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6876</th>\n",
       "      <td>\\nI'm for the moment interested in this notion of the 'leap of faith'\\nestablished by Kierkegaard. It clearly points out a possible solution\\nto transcendental values. What I don't understand is t...</td>\n",
       "      <td>moment interest notion leap faith establish kierkegaard clearly point possible solution transcendental value what understand also clearly show existentialism system where leap transcendental direc...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>0.855462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11825</th>\n",
       "      <td>\\nI think that if a theist were truly objective and throws out the notion that\\nGod definitely exists and starts from scratch to prove to themselves that\\nthe scriptures are the whole truth then t...</td>\n",
       "      <td>think theist truly objective throw notion god definitely exist start scratch prove scriptures whole truth person would longer theist miss something people who convert non theism theism bring non t...</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>0.844723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                       Article  \\\n",
       "2467   Why is the NT tossed out as info on Jesus.  I realize it is normally tossed\\nout because it contains miracles, but what are the other reasons?\\n\\nMAC\\n--\\n*****************************************...   \n",
       "5746   : In my mind, to say that science has its basis in values is a bit of a\\n: reach. Science has its basis in observable fact. \\n\\nI'd say that what one chooses to observe and how the observation is\\...   \n",
       "11119                                                                                                    \\n\\n\\tUnless God admits that he didn't do it....\\n\\n\\t=)\\n\\n\\n---  \\n\\n  \" I'd Cheat on Hillary Too.\"   \n",
       "6876   \\nI'm for the moment interested in this notion of the 'leap of faith'\\nestablished by Kierkegaard. It clearly points out a possible solution\\nto transcendental values. What I don't understand is t...   \n",
       "11825  \\nI think that if a theist were truly objective and throws out the notion that\\nGod definitely exists and starts from scratch to prove to themselves that\\nthe scriptures are the whole truth then t...   \n",
       "\n",
       "                                                                                                                                                                                                 Clean Article  \\\n",
       "2467   why nt toss info jesus realize normally toss contain miracles what reason mac michael cobb raise tax middle university illinois class pay program champaign urbana bill clinton 3rd debate cobb alex...   \n",
       "5746   mind say science basis value bite reach science basis observable fact say what one choose observe how observation interpret what significance give depend great deal value observer science human ac...   \n",
       "11119                                                                                                                                                                           unless god admit cheat hillary   \n",
       "6876   moment interest notion leap faith establish kierkegaard clearly point possible solution transcendental value what understand also clearly show existentialism system where leap transcendental direc...   \n",
       "11825  think theist truly objective throw notion god definitely exist start scratch prove scriptures whole truth person would longer theist miss something people who convert non theism theism bring non t...   \n",
       "\n",
       "       Target Label         Target Name Predicted Name  Predicted Confidence  \n",
       "2467             19  talk.religion.misc    alt.atheism              0.999882  \n",
       "5746             19  talk.religion.misc    alt.atheism              0.992743  \n",
       "11119            19  talk.religion.misc    alt.atheism              0.893857  \n",
       "6876             19  talk.religion.misc    alt.atheism              0.855462  \n",
       "11825            19  talk.religion.misc    alt.atheism              0.844723  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 200)\n",
    "res_df = (test_df[(test_df['Target Name'] == 'talk.religion.misc') & (test_df['Predicted Name'] == 'alt.atheism')]\n",
    "       .sort_values(by=['Predicted Confidence'], ascending=False).head(5))\n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d429be61",
   "metadata": {},
   "source": [
    "This should be a no-brainer considering atheism and religion are related in several aspects when people talk about them, especially on online forums. You should check if there are some other interesting patterns in the articles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71788270",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Compute the performance metrics of the various machine learning techniques. Consider the `model_evaluation_utils.py` file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
